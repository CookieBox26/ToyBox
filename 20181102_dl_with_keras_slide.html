<html>

<head>
  <title>20181102</title>
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
  </script>
  
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
  </script>
  
  <script>
    var currentPage = 0;
    var lastPage = 0;
    
    function ShowPage(n) {
      for (i = 1; i < 100; ++i) {
        var id = "p" + i;
        var elm = document.getElementById(id);
        if (i == n) {
          elm.style.display="block";
          continue;
        }
        if (elm) {
          elm.style.display="none";
        } else {
          lastPage = i - 1;
          break;
        }
      }
      document.getElementById("pageMark").innerHTML = n + " / " + lastPage + " ページ";
    }
    
    function MoveTo(n) {
      if (currentPage != n) {
        ShowPage(n);
        currentPage = n;
      }
    }
    function MoveBack() {
      if (currentPage != 1) {
        MoveTo(currentPage - 1);
      }
    }
    function MoveNext() {
      if (currentPage != lastPage) {
        MoveTo(currentPage + 1);
      }
    }
    window.onload = function() {
      ShowPage(1);
      currentPage = 1;
    }
    var sectionHeadArray = new Array();
    sectionHeadArray[0] = 1;
    function SetSectionHead(n_section, n) {
      sectionHeadArray[n_section] = n;
    }
    function MoveToSection(n_section) {
      MoveTo(sectionHeadArray[n_section]);
    }
  </script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: #4682b4; //#4169e1;
     //color: rgb(104, 118, 135);
     //color: #a4c1d7;//#8da0b6;//#b0c4de;
   }

   pre .literal {
     color: #ff8c00;
     //color: #dcdcdc;
   }

   pre .number {
     color: #a52a2a; //#099;
     //color: #ff7f7f;//#ff69b4;//#ffb6c1;
   }

   pre .comment {
     color: #2e8b57; //#998;
     //color: #9acd32;//#7fff7f;//#00ff00;//#9cbb1c;//#9acd32;
     font-style: italic
   }

   pre .keyword {
     //color: #900;
     color: #4169e1;
     //color: #7fbfff;//#87cefa;
     //font-weight: bold
   }

   pre .identifier {
     color: #333;
     //color: rgb(0, 0, 0);
     //color: #dcdcdc;
   }

   pre .string {
     color: #d2691e;//#ffa500;
   }
   table.a td{
     text-align: center;
     vertical-align: middle;
     width: 30px;
   }
div.take {
	background-color: #cee4ae;
	padding: 0 0.5em 0 0;
	border-left: 0em solid #cee4ae;
	border-top: 0.7em solid #cee4ae;
	border-right: 0em solid #cee4ae;
	border-bottom: 0.7em solid #cee4ae;
	margin: 0.2em 0 0.5em 0;
}
div.c1e4e9 {
	background-color: #c1e4e9;
	padding: 0 0.5em 0 0;
	border-left: 0em solid #c1e4e9;
	border-top: 0.7em solid #c1e4e9;
	border-right: 0em solid #c1e4e9;
	border-bottom: 0.7em solid #c1e4e9;
	margin: 0.2em 0 0.5em 0;
}
div.ddd {
	background-color: #ddd;
	padding: 0 0.5em 0 0;
	border-left: 0em solid #ddd;
	border-top: 0.7em solid #ddd;
	border-right: 0em solid #ddd;
	border-bottom: 0.7em solid #ddd;
	margin: 0.2em 0 0.5em 0;
}
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass|import|from|print|class|def|self)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<style type="text/css">
body {
  font-size: 139%;
  font-family: 'Tahoma','メイリオ',sans-serif;
  color: #4d4d4d;
  letter-spacing: 0.06em;
  width: 920px;
}
table td {
  font-size: 127%;
}
a {
  color: teal;
  text-decoration: none;
}
tt, code, pre {
  font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}
pre code {
  display: block;
  margin-top: -0.3em;
  margin-bottom: -0.2em;
  padding: 0.5em 0.7em;
}
code {
  font-size: 84%;
  border: 0px solid #ccc;
  color: #dcdcdc;
  background-color: #4d4d4d;
  line-height: 1.2;
}
code[class] {
  background-color: #F0F0F0;
  color: #4d4d4d;
}
div.slide {
  margin: 34px;
}
ul {
  margin-top: 0.5em;
  margin-bottom: 0.4em;
}
ol {
  margin-top: 0.3em;
}
h1 {margin: 0.7em 0 0.4em;}
h2 {margin: 0.7em 0 0.4em;}
h3 {margin: 0.7em 0 0.4em;}
h4 {margin: 0.7em 0 0.4em;}
</style>
  
</head>

<body>

<div style="padding: 0.4em 0.6em 0.1em; font-size: 105%;">
<a href='javascript:void(0)' onclick='MoveTo(1)'>最初</a> 
<a href='javascript:void(0)' onclick='MoveBack()'>前</a>
<a href='javascript:void(0)' onclick='MoveNext()'>次</a>
<a href='javascript:void(0)' onclick='MoveTo(lastPage)'>最後</a>
<span id='pageMark' style="float:right; padding-right: 2px;"></span></div>
<hr style="border-top: 2px solid #4d4d4d;"/>
<!--
<a href='javascript:void(0)' onclick='MoveToSection(1)'></a>
  <script>
    SetSectionHead(1, 3);
  </script>
-->


<!-- p1 --->
<div id='p1' class='slide'>
  <h1>直感 Deep Learning 5章 単語分散表現<br/>を読んで面白いことがしたかった話</h1>
  <h3>テキスト</h3>
  <table  border="0" cellpadding="5"><tr><td valign="top" style="border:0"><a href="http://www.amazon.co.jp/exec/obidos/ASIN/4873118263/goodpic-22/" target="_top"><img src="https://images-fe.ssl-images-amazon.com/images/I/51W34ZKKlHL._SL160_.jpg" border="0" alt="直感 Deep Learning ―Python×Kerasでアイデアを形にするレシピ" /></a></td><td valign="top" style="border:0"><font size="-1"><a href="http://www.amazon.co.jp/exec/obidos/ASIN/4873118263/goodpic-22/" target="_top">直感 Deep Learning ―Python×Kerasでアイデアを形にするレシピ</a><br />Antonio Gulli Sujit Pal 大串 正矢 <br /><br />オライリージャパン  2018-08-11<br />売り上げランキング : 1992<br /><br /><a href="http://www.amazon.co.jp/exec/obidos/ASIN/4873118263/goodpic-22/" target="_top">Amazonで詳しく見る</a></font><font size="-2"> by <a href="http://www.goodpic.com/mt/aws/index.html" >G-Tools</a></font></td></tr></table>
  <h3>その他の参考文献</h3>
  <ol>
    <li><a href="http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/">日本語 Wikipedia エンティティベクトル（東北大学 乾・岡崎研究室）</a></li>
  </ol>
</div>




<!-- p2 --->
<div id='p2' class='slide'>
  <h2>単語分散表現とは（１）</h2>
  <ul>
    <li>ニューラルネットに単語や文章（＝単語列）を入力したいことがある。</li>
    <center><table style="margin: 0.8em;"><tr><td style="text-align:center">映画のレビュー<br/>の文章</td><td>　\(\to\)　</td><td style="border: 1px solid #4d4d4d;" width="100" height="60">　</td><td>　\(\to\)　</td><td style="text-align:center">ネガティブかポジティブか<br/>（ 0 or 1 ）</td></tr></table></center>
    <li>単語をニューラルネットに入れることはできないので、各単語を実数（ベクトル）に変換したい。</li>
    <li>各単語に適当に番号を付けるのは1つの方法（Ex．辞書順や登場頻度順に並べて 1, 2, 3, … とする）だが、単語というのは意味があるものなので、もう少し何か単語の意味を反映したような変換をした方がよさそうな気がする。</li>
  </ul>
  なので<b>単語分散表現</b>（＝ 単語集合から実ベクトル空間への写像であって、単語どうしの何らかの関係性を反映したもの）がほしい。
</div>



<!-- p3 --->
<div id='p3' class='slide'>
  <h3>単語分散表現の例</h3>
  参考文献 1. で公開されている、日本語 Wikipedia で記事となっているエンティティの100次元表現（Skip-gram で学習）。
  <a style="font-size:94%" href="https://github.com/singletongue/WikiEntVec">https://github.com/singletongue/WikiEntVec</a>
    \[
      f({\large ドーナツ}) = \left(
  \begin{array}{c}
      0.15019251   \\
      0.4490861 \\
      \vdots \\
      -0.5708073
    \end{array}
  \right), \; \; \;
      f({\large クッキー}) = \left(
  \begin{array}{c}
      -0.04867858   \\
      0.44743043 \\
      \vdots \\
       -0.5990641
    \end{array}
  \right)
    \]
    \[
      {\rm cos} \bigl(f({\large ドーナツ}) , \; f({\large クッキー}) \bigr) = 0.8620
    \]
    \[
      {\rm cos} \bigl(f({\large ドーナツ}) , \; f({\large ラーメン}) \bigr) = 0.6936
    \]
<pre><code class="r">from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('entity_vectors.txt')
print(model[u'ドーナツ'])
print(model[u'クッキー'])
print(model.similarity(u'ドーナツ', u'クッキー'))
print(model.similarity(u'ドーナツ', u'ラーメン'))</code></pre>
</div>



<!-- p4 --->
<div id='p4' class='slide'>
  <h2>単語分散表現とは（２）</h2>
  <ul>
    <li>といっても単語どうしの何らかの関係性を反映した表現とは明らかでない。「意味が近い単語のベクトルどうしは、何らかの距離が近い」ような表現であってほしいが、意味が近いとは何かということになる。<span style="font-size: 100%; color: teal;">「ドーナツ」と「クッキー」は近いかもしれないが、何がそれを規定する？</span></li>
    <li>そこで単語分散表現（の少なくとも Skip-gram, CBOW, GloVe）では、コーパス（文章の集合）を利用し、ある単語とある単語が「文章中で N 単語以内の距離に登場するか」という特徴を学ぶのに適した表現を得ようとする。</li>
    <center><table style="margin: 0.8em;"><tr><td style="text-align:center">コーパス</td><td>　\(\to\)　</td><td style="border: 1px solid #4d4d4d;" width="100" height="60">　</td><td>　\(\to\)　</td><td style="text-align:center">単語分散表現 \(f\)</td></tr></table></center>
    <ul>
      <li>Skip-gram ― 「ある単語と別のある単語を入力して、文章内で近くに登場するペアか否か出力する」をニューラルネットで学習して、重みを取り出す（とそれが単語分散表現になっている）。</li>
      <li>CBOW ― 「ある単語群を入力して、どの単語がその中心にありそうかを出力する」をニューラルネットで学習して、重みを取り出す。</li>
      <li>GloVe ― 各行が単語、各列が単語列（謎）になっていて各要素がそれらが共起した回数（おそらく）になっているような行列を用意して、2つの行列の積に分解する（と1つ目の行列が単語分散表現になっている）。</li>
    </ul>
  </ul>
</div>



<!-- p5 --->
<div id='p5' class='slide'>
  <h2>Skip-gram による単語分散表現の作り方</h2>
  <ol>
    <li>以下のようなニューラルネットワークを組んで、「ある単語（中心語）について、他のある単語（文脈語）が文章中で N 単語以内の距離に登場するか否か（1 or 0）」を学習する。</li>
    <div style="border: 1px solid #4d4d4d; padding: 0.4em; margin: 0.5em 0 0.7em"><ol start="0">
      <li>中心語と文脈語のペアを入力する。</li>
      <li>中心語を中心語用の Embedding 層で数値ベクトルにする（★）。</li>
      <li>文脈語を文脈語用の Embedding 層で数値ベクトルにする。</li>
      <li>1. と 2. の内積をとる。</li>
      <li>3. に出力の次元数が1の Dense 層をかぶせ、sigmoid で活性化して最終的な判定（1 or 0）とする。</li>
    </ol></div>
    <li>（★）が得たい単語分散表現に他ならない。</li>
  </ol>
</div>



<!-- p6 --->
<div id='p6' class='slide'>
  <h3>Keras による Skip-gram 識別器の実装例</h3>
  「単語数」、「何次元に埋め込むか」さえ決めればネットワーク構造が決まる。
<pre><code class="r"># -*- coding: utf-8 -*-
import numpy as np
from keras.layers import Dot, Input, Dense, Reshape, Embedding
from keras.models import Model

class SkipGramDiscriminator():
    def __init__(self, vocab_size, embed_size):
        self.vocab_size = vocab_size # 語彙数
        self.embed_size = embed_size # 埋め込み次元数
    def create_model(self):
        # 中心語ID --> 中心語数値ベクトル表現
        x0 = Input(shape=(1,))
        y0 = Embedding(self.vocab_size, self.embed_size,
                       embeddings_initializer='glorot_uniform')(x0)
        y0 = Reshape((self.embed_size,))(y0)
        self.word_embedder = Model(x0, y0)
        # 文脈語ID --> 文脈語数値ベクトル表現
        x1 = Input(shape=(1,))
        y1 = Embedding(self.vocab_size, self.embed_size,
                       embeddings_initializer='glorot_uniform')(x1)
        y1 = Reshape((self.embed_size,))(y1)
        self.context_embedder = Model(x1, y1)
        # 内積 --> ロジスティック回帰
        y = Dot(axes=-1)([y0, y1])
        y = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(y)
        self.discriminator = Model(inputs=[x0, x1], outputs=y)
        self.discriminator.compile(loss='mean_squared_error', optimizer='adam')
        print(self.discriminator.summary())
    
if __name__ == '__main__':
    sg = SkipGramDiscriminator(6, 3) # i,love,green,eggs,and,ham 6語を3次元空間へ埋込
    sg.create_model()
    
    x0 = np.array([[1], [4], [1], [4], [2]]) # 中心語： love,and,love,and,green
    x1 = np.array([[0], [5], [2], [2], [2]]) # 文脈語： i,ham,green,green,green
    y  = np.array([[1], [1], [1], [0], [0]]) # 正解ラベル
    sg.discriminator.fit([x0, x1], y, epochs=1000) # 学習
    
    y_pred = sg.discriminator.predict([x0, x1])
    print(y_pred) # 中心語と文脈語のペアであるかどうかの判定結果

    # 中心語の数値ベクトル表現は中心語の Embedding 層の重みそのもの
    print(sg.word_embedder.get_weights())

    # IDから数値ベクトル表現を取り出せることの確認
    print(sg.word_embedder.predict([[0]])) # i の数値ベクトル表現
    print(sg.word_embedder.predict([[1]])) # love の数値ベクトル表現</code></pre>
<pre><code>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 1)            0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 1)            0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 1, 3)         18          input_1[0][0]
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 1, 3)         18          input_2[0][0]
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 3)            0           embedding_1[0][0]
__________________________________________________________________________________________________
reshape_2 (Reshape)             (None, 3)            0           embedding_2[0][0]
__________________________________________________________________________________________________
dot_1 (Dot)                     (None, 1)            0           reshape_1[0][0]
                                                                 reshape_2[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            2           dot_1[0][0]
==================================================================================================
Total params: 38
Trainable params: 38
Non-trainable params: 0
__________________________________________________________________________________________________
...
Epoch 1000/1000
5/5 [==============================] - 0s 397us/step - loss: 0.0020
[[0.9564351 ]
 [0.94431126]
 [0.9720375 ]
 [0.05245696]
 [0.03800274]]
[array([[-0.13738829, -0.79445064, -0.73896384],
       [-0.8272589 , -0.03786828, -0.72830474],
       [ 0.6510331 ,  0.11700394,  0.94733346],
       [ 0.4470725 , -0.807297  , -0.64195573],
       [ 0.6196652 , -0.00116872,  0.8576324 ],
       [ 0.4181409 ,  0.26542222,  0.7537674 ]], dtype=float32)]
[[-0.13738829 -0.79445064 -0.73896384]]
[[-0.8272589  -0.03786828 -0.72830474]]</code></pre>
</div>



<!-- p7 --->
<div id='p7' class='slide'>
ネットワークはできたが、問題は学習データの準備。<br/>
日本語のテキストを学習してみたいが、テキストは元々英語の本なので日本語の取り扱いについて書いていない。<br/>
\(\to\) 今回やった学習データ準備の手順は以下<span style="color: teal;">（全体的によくわからない）</span>。<br/>
<ul>
  <li>日本語コーパスの文章を1文ずつ MeCab で単語毎の半角空白区切りにする。</li>
  <li>そうなるともう英語と同じなので、各文に対してテキスト同様 keras の関数で ID をふり、Skip-gram 対（正例については全てと、負例は正例と同数サンプリング）を生成する（143ページ）。</li>
  <li>ただし、文章ごとに負例をサンプリングすると、他の文章で正例であるペアが含まれているかもしれない。ので、後から負例を全部チェックして、正例と同じペアがあったら除く（テキストは1文しかなくよくわからない）。</li>
</ul>
とにかくこの手順で、以下のドーナツ好きな2人のゲームキャラクターのセリフをそれぞれ学習して、ドーナツの学習結果がどのようになるか見てみたい。
<ul>
  <li><a href="https://imascg-slstage-wiki.gamerch.com/%E6%A4%8E%E5%90%8D%E6%B3%95%E5%AD%90">椎名法子 -アイマス デレステ攻略まとめwiki【アイドルマスター シンデレラガールズ スターライトステージ】 - Gamerch</a></li>
  <li><a href="https://wikiwiki.jp/sidem/%E8%8B%A5%E9%87%8C%20%E6%98%A5%E5%90%8D">若里 春名 - アイドルマスターsideM Wiki*</a></li>
</ul>
セリフはあらかじめ上の URL からパースして、半角を全角にするプレ処理のみしてある。

<pre><code class="r"># -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from keras.layers import Dot, Input, Dense, Reshape, Embedding
from keras.models import Model
from keras.preprocessing.text import *
from keras.preprocessing.sequence import skipgrams
import MeCab
import codecs
from sklearn.manifold import TSNE
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties

# Skip-gram ペア収集クラス
class SkipGramCollector():
    def __init__(self, list_text_raw, window_size):
        self.filters = '～？♪○！､….!?、。「」『』\n\r'
        self.tagger = MeCab.Tagger('-Owakati')
        self.window_size = window_size # 周辺の何単語を考慮するか
        self.stop_words = []
        for w in open('Japanese.txt', 'r'): # SlothLib の StopWords
            w = w.replace('\n', '')
            if len(w) > 0:
                self.stop_words.append(w)
        
        # 形態素解析し全単語を収集して、単語-->ID辞書と、ID-->単語辞書を作成
        self.list_text = []
        for text_raw in list_text_raw:
            text = self.tagger.parse(text_raw)
            # ストップワード除去
            _text = text.split()
            _text = [_t for _t in _text if (_t not in self.stop_words)]
            text = ' '.join(_text)
            self.list_text.append(text)
        self.tokenizer = Tokenizer(filters=self.filters)
        self.tokenizer.fit_on_texts(self.list_text)
        self.word2id = self.tokenizer.word_index
        self.id2word = {v:k for k, v in self.word2id.items()}
        
        # 文章ごとに Skip-gram 対を得る
        df = pd.DataFrame()
        for text in self.list_text:
            wids = [self.word2id[w] for w in text_to_word_sequence(text, filters=self.filters)]
            pairs, labels = skipgrams(wids, len(self.word2id), window_size=self.window_size)
            df_ = pd.DataFrame()
            df_['x0'] = np.array([pair[0] for pair in pairs]).astype(np.int64) # 中心語
            df_['x1'] = np.array([pair[1] for pair in pairs]).astype(np.int64) # 文脈語
            df_['y'] = np.array(labels).astype(np.int64) # 正解ラベル
            df = pd.concat([df, df_])
        
        # 負例にサンプリングされた Skip-gram 対のうち正例に含まれているものを除去
        df_dup_check = df[df.duplicated(keep=False) & (df.y == 0)] # 複数ある対であって負例
        df_dup_check = df_dup_check[df_dup_check.duplicated() == False].copy()
        for index, row in df_dup_check.iterrows():
            df_temp = df[(df.x0 == row['x0']) & (df.x1 == row['x1'])]
            if np.sum(df_temp.y == 1) > 0: # 正例があるのでこの対の負例からは削除する
                df_temp = df_temp[df_temp.y == 0]
                df = df.drop(index=df_temp.index)
        df.reset_index(drop=True, inplace=True)
        print("正例の数：", np.sum(df.y == 1))
        print("負例の数：", np.sum(df.y == 0))
        self.df = df

# Skip-gram 識別器クラス
class SkipGramDiscriminator():
    def __init__(self, vocab_size, embed_size):
        self.vocab_size = vocab_size # 語彙数
        self.embed_size = embed_size # 埋め込み次元数
    
    def create_model(self):
        # 中心語ID --> 中心語数値ベクトル表現
        x0 = Input(shape=(1,))
        y0 = Embedding(self.vocab_size, self.embed_size,
                       embeddings_initializer='glorot_uniform')(x0)
        y0 = Reshape((self.embed_size,))(y0)
        self.word_embedder = Model(x0, y0)
        
        # 文脈語ID --> 文脈語数値ベクトル表現
        x1 = Input(shape=(1,))
        y1 = Embedding(self.vocab_size, self.embed_size,
                       embeddings_initializer='glorot_uniform')(x1)
        y1 = Reshape((self.embed_size,))(y1)
        self.context_embedder = Model(x1, y1)
        
        # 内積 --> ロジスティック回帰
        y = Dot(axes=-1)([y0, y1])
        y = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(y)
        self.discriminator = Model(inputs=[x0, x1], outputs=y)
        self.discriminator.compile(loss='mean_squared_error', optimizer='adam')
        print(self.discriminator.summary())

if __name__ == '__main__':
    _train = False # 単語分散表現を学習する
    _plot = True # 結果をプロットする
    who = 'haruna' # 'noriko'
    
    if _train:
        # ===== 学習用 Skip-gram 対の生成 =====
        df = pd.read_csv(who + '.csv') # 'str' というカラムにセリフが入っているデータフレーム
        print(df['str'].head(3))
        sgc = SkipGramCollector(df['str'].values, 2)
        print(sgc.df.shape)
        np.savetxt('word_' + who + '.csv', np.array([k for k, v in sgc.word2id.items()]),
                   delimiter=',', fmt='%s')
        
        print('\r\n----- 単語登場回数 (ユニーク単語数：' + str(len(sgc.word2id)) +
             ', 語数：' + str(sum([v[1] for v in sgc.tokenizer.word_counts.items()])) + ') -----')
        print(sorted(sgc.tokenizer.word_counts.items(), key=lambda x:x[1], reverse=True)[0:25])
        
        print('\r\n----- 文書ベース単語登場回数 (文書数：' + str(len(sgc.list_text)) + ') -----')
        print(sorted(sgc.tokenizer.word_docs.items(), key=lambda x:x[1], reverse=True)[0:25])
        
        print('\r\n----- Skip-gram対の例 -----')
        for index, row in sgc.df.iterrows():
            print("({:s} ({:d}), {:s} ({:d})) -> {:d}".format(
              sgc.id2word[row['x0']], row['x0'], sgc.id2word[row['x1']], row['x1'], row['y']))
            if index == 10:
                break
        
        # ===== ネットワークの学習 =====
        sg = SkipGramDiscriminator(len(sgc.word2id), 4) # 4次元に埋め込む場合
        sg.create_model()
        sg.discriminator.fit([sgc.df.x0.values, sgc.df.x1.values], sgc.df.y.values,
                             batch_size=32, epochs=100)
        weight = sg.word_embedder.get_weights()[0]
        print(weight.shape)
        np.savetxt('weight_' + who + '.csv', weight, delimiter=',')
    
    if _plot:
        # ===== 2次元に次元削減してプロット =====
        fp = FontProperties(fname=r'C:\WINDOWS\Fonts\YuGothB.ttc', size=13)
        weight = np.loadtxt('weight_' + who + '.csv', delimiter=',')
        words = np.loadtxt('word_' + who + '.csv', delimiter=',', dtype='unicode')
        print(weight.shape)
        weight2 = TSNE(n_components=2, random_state=0).fit_transform(weight)
        print(weight2.shape)
        plt.scatter(weight2[:,0], weight2[:,1], c='darkgray')
        for word in (['オレ', 'プロデューサー', 'ドーナツ', 'ドラム'] if who is 'haruna' else \
                     ['あたし', 'プロデューサー', 'ドーナツ', 'アイドル']):
            i = np.where(words == word)
            plt.scatter(weight2[i,0], weight2[i,1], c='black')
            plt.text(weight2[i,0], weight2[i,1], word, fontproperties=fp)
        plt.savefig('figure_' + who + '.png')</code></pre>
<h4>noriko</h4>
<pre><code>正例の数： 3966
負例の数： 3901
(7867, 3)

----- 単語登場回数 (ユニーク単語数：902, 語数：3238) -----
[('の', 132), ('て', 110), ('に', 108), ('は', 90), ('プロデューサー', 89),
 ('ドーナツ', 72), ('た', 69), ('だ', 65), ('で', 58), ('ー', 57), 
 ('も', 56), ('よ', 53), ('っ', 52), ('ね', 49), ('と', 45), 
 ('あたし', 42), ('し', 41), ('を', 41), ('か', 39), ('が', 37), 
 ('な', 37), ('お', 32), ('ない', 31), ('ん', 25), ('食べ', 20)]

----- 文書ベース単語登場回数 (文書数：309) -----
[('の', 115), ('に', 95), ('プロデューサー', 89), ('て', 89), ('は', 81), 
 ('ドーナツ', 70), ('た', 61), ('だ', 57), ('で', 54), ('よ', 50), 
 ('も', 50), ('ね', 49), ('ー', 47), ('っ', 45), ('あたし', 40), 
 ('し', 40), ('と', 38), ('か', 37), ('を', 37), ('が', 35), 
 ('な', 33), ('お', 31), ('ない', 29), ('ん', 23), ('食べ', 19)]

----- Skip-gram対の例 -----
(ごちそうさま (336), た (7)) -> 1
(ー (10), プロデュース (738)) -> 0
(ー (10), い (40)) -> 1
(でし (337), ごちそうさま (336)) -> 1
(でし (337), ブレスレット (724)) -> 0
(い (40), がり (475)) -> 0
(い (40), 聞い (288)) -> 0
(い (40), かける (866)) -> 0
(でし (337), 負け (850)) -> 0
(ごちそうさま (336), 手作り (234)) -> 0
(た (7), でし (337)) -> 1</code></pre>
<center><img src="img/figure_noriko.png"/></center><br/>
<h4>haruna</h4>
<pre><code>正例の数： 4378
負例の数： 4231
(8609, 3)

----- 単語登場回数 (ユニーク単語数：1181, 語数：5425) -----
[('て', 232), ('の', 220), ('な', 195), ('に', 181), ('だ', 155),
 ('が', 112), ('は', 111), ('も', 111), ('オレ', 91), ('よ', 91),
 ('た', 90), ('し', 87), ('と', 85), ('で', 82), ('ん', 81), 
 ('プロデューサー', 75), ('って', 75), ('ぜ', 74), ('か', 69), ('ない', 66), 
 ('を', 66), ('ドーナツ', 57), ('さ', 47), ('てる', 43), ('いい', 42)]

----- 文書ベース単語登場回数 (文書数：286) -----
[('な', 161), ('て', 154), ('の', 153), ('に', 131), ('だ', 128), 
 ('が', 96), ('も', 96), ('オレ', 88), ('よ', 88), ('は', 86), 
 ('で', 77), ('し', 75), ('ぜ', 74), ('と', 73), ('ん', 73), 
 ('た', 73), ('プロデューサー', 71), ('って', 66), ('か', 64), ('ない', 62), 
 ('を', 62), ('ドーナツ', 48), ('さ', 45), ('てる', 40), ('いい', 40)]

----- Skip-gram対の例 -----
(ない (20), で (14)) -> 1
(なら (44), が (6)) -> 1
(なら (44), 作る (478)) -> 1
(の (2), 時代 (755)) -> 0
(悪く (483), 減っ (1008)) -> 0
(きく (484), こ (397)) -> 0
(許し (287), ねだり (732)) -> 0
(の (2), コーヒー (161)) -> 1
(悪く (483), ない (20)) -> 1
(は (7), 甘酒 (365)) -> 0
(悪く (483), と (13)) -> 1</code></pre>
<center><img src="img/figure_haruna.png"/></center><br/>
よくわからなかった。<br/>
そもそも登場頻度が多い語が「の」とか「て」なのが駄目そうな気がする。<br/>
データも少ないと思うし埋め込む次元数もよくわからない。<br/>
でもせっかくなので gensim で読める形式にして、「ドーナツ」に類似の単語を出してみる。<br/>
<pre><code class="r"># -*- coding: utf-8 -*-
import numpy as np
from gensim.models import KeyedVectors

if __name__ == '__main__':
    who = 'noriko'
    with open('word_weight_' + who + '.txt', mode='w', encoding='utf-8') as f:
        words = np.loadtxt('word_' + who + '.csv', delimiter=',', dtype='unicode')
        f.write(str(len(words)) + ' ' + '4\n')
        i = 0
        weight = open('weight_' + who + '.csv', 'r')
        for line in weight:
            f.write(words[i] + ' ' + line.replace(',', ' '))
            i = i + 1
        weight.close()
    
    model = KeyedVectors.load_word2vec_format('word_weight_' + who + '.txt')
    print(len(model.vocab))
    print(model[u'ドーナツ'])
    print(model.most_similar(u'ドーナツ'))</code></pre>
<h4>noriko</h4>
<pre><code>902
[-0.37491137 -0.10871826  1.0103428   0.5701179 ]
[('くる', 0.9954972863197327),
 ('だろ', 0.9908122420310974), 
 ('来', 0.9868786931037903), 
 ('おっかな', 0.9773104190826416), 
 ('忙しい', 0.9730000495910645), 
 ('お', 0.9729252457618713), 
 ('全力', 0.9648570418357849), 
 ('ぶ', 0.9618979692459106), 
 ('ねえ', 0.9618968367576599), 
 ('もっ', 0.959823489189148)]</code></pre>
<h4>haruna</h4>
<pre><code>1181
[ 0.07973216 -0.8829228   0.758757   -0.4895581 ]
[('足', 0.990249752998352), 
 ('いっちょ', 0.9864041209220886), 
 ('プロ', 0.9842278957366943), 
 ('教科書', 0.9799279570579529), 
 ('仲間', 0.9799133539199829), 
 ('声', 0.978289008140564), 
 ('頼っ', 0.9779394865036011), 
 ('ホワイト', 0.9706403613090515), 
 ('いき', 0.9678006768226624), 
 ('いっ', 0.9661396741867065)]</code></pre>
</div>



<!-- p8 --->
<div id='p8' class='slide'>
<h3>わからなかったこと</h3>
<ul>
  <li>前処理の仕方が全然わかっていない。助詞をどうやって除いているのかよくわからない。頻度だと思う。</li>
  <li>よい単語分散表現が得られたというのはどうやってわかるのか。でも、単語分散表現は最終目的ではないはずなので、その単語分散表現を用いて解きたいタスクの精度が上がったらよいのだと思う。</li>
</ul>

<h3>5章の内容のつづき</h3>
<ul>
  <li>CBOW も keras で実装できる。</li>
  <li>でも Skip-gram や CBOW を学習したいなら gensim パッケージがよい。</li>
  <li>GloVe についてはツールの紹介のみ。</li>
  <li>あと実際に自然言語処理タスクを解くときに3パターンあるという話。</li>
  <ul>
    <li>特に Skip-gram, CBOW, GloVe 的なアプローチを取らずランダムな初期値から Embedding 層を含め学習する。</li>
    <li>特に Skip-gram, CBOW, GloVe 等で学習済みの表現を予めセットして Embedding 層を含め学習する。</li>
    <li>特に Skip-gram, CBOW, GloVe 等で学習済みの表現を予めセットして Embedding 層は学習しない。</li>
  </ul>
</ul>

<h3>試行錯誤の形跡</h3>
<ul>
  <li><a href="http://cookie-box.hatenablog.com/entry/2018/09/15/181113">直感 Deep Learning： 5章メモ（単語分散表現）（その1）</a></li>
  <li><a href="http://cookie-box.hatenablog.com/entry/2018/10/01/235641">直感 Deep Learning： 5章メモ（単語分散表現）（その2）</a></li>
  <li><a href="http://cookie-box.hatenablog.com/entry/2018/10/14/184801">直感 Deep Learning： 5章メモ（単語分散表現）（その3）</a></li>
  <li><a href="http://cookie-box.hatenablog.com/entry/2018/10/30/095055">直感 Deep Learning： 5章メモ（単語分散表現）（番外編）</a></li>
</ul>
</div>



</body>

</html>
